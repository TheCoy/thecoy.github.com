---
title: 一个bug的来龙去脉
date: 2019-06-21 14:07:28
tags: 杂谈
---

# 一个报警的连锁反应

## 背景

> 2019年06月20日，又是一个风(bai)和(wu)日(liao)丽(lai)的日子，早上刚到公司，刷会手机喝喝水准备开站会的前夕，突然收到一封报警邮件。尽管早已习惯报警邮件的晚高峰轰炸，但头一次在美好的早晨被报警邮件所叨扰，还是不免心头一紧。

## 似曾相识

然后开完站会，就发现这个报警和上次的似乎差不多。具体报警内容是这样的：

```shell
PHP Fatal error:  Allowed memory size of 67108864 bytes exhausted (tried to allocate 20480 bytes)
```

之前也出现过一次相同的内存爆掉的问题，而且是同样的文件，同样的位置。当时定位到的问题根源是这样的：
1. QA自动化在对线上的接口作测试，其中就包括了这一个接口，但是它的测试用例集中在了某一个具体的caseID，导致这个caseID下的数据量产生异常
2. 以往业务并没有考虑数据量异常的情形，因此在对DB库做query的时候并没有做limit限制，进而导致后续的列表Query在操作处理时直接内存爆掉了
3. 此时自动化脚本还在不停地"刷"这个接口，而且最要命的是，与此同时还有一个第三方服务在异步消费过程中也会query相关的caseID数据列表，而由于内存爆掉，进程异常退出，所以消费方认为不成功，于是不停地疯狂重试，结果就导致了错误日志不停地在刷

所以，从问题的产生的表象看：
QA自动化测试的骚操作 -> 数据库数据量异常 -> 数据列表Query异常 -> 第三方消费异常 -> 报警

从问题的产生根源看：
产品细节que的不完整 -> 代码逻辑设计不够健壮 -> 随异常数据产生开始曝露代码BUG -> 报警

## 反反复复

继第一次的报警之后，当时分析的是这种情况属于个例问题，于是就针对特定的caseID做了屏蔽处理。当时考虑的是正常的业务场景是不会出现这种数据量异常的大量情形，而且针对该类问题的查询确实有一次query整个列表的使用场景在，于是当时就是这么处理过去了。

然后，这一次一开始报警的时候我又以为是一些自动化测试开始搞事情了，于是我想要杜绝这类问题再次发生，只能去做保证和限制了。因为永远不要相信第三方是一名后端工程师必备的常识。于是先代码加上Limit限制再去追责吧。

然而问题的处理并没有想象中那么顺利：
1. 代码改动比较慌乱，导致引入了一些其它异常出现
2. 代码改动时发现了代码规范的问题，其实如果代码写的好一些，应该也不会出现问题1
3. 临时的方案尚有缺陷，并没有兼顾到所有场景，需要后面持续改进优化

总之，伴随着上述的问题跌跌撞撞中算是解决了线上的报警问题。OK，接下来开始分析和追查问题产生的原因：
1. 分析日志找到了caseID，但发现竟然和上次的凶手不是同一个，并且和上任凶手确认过确实不是他们的锅。嗯，我承认之前冤枉了他们
2. 翻查相关case的数据记录，发现请求虽然频率较低（约1次/秒），但仍然可认为是有恶意的，尽管不知道是出于什么目的。
3. 分析相关联用户的利益相关数据，发现了用户的财富记录不太正常，狐狸尾巴终于还是露出来了。
4. 既然是为了刷财富，那必然是有BUG，那么就审查相关代码吧。果不其然，经过代码查阅发现是之前的一个签到任务引起的BUG，用户可以通过重复请求这个接口从而达到刷财富值的目的。


## 细思恐极

好了，问题的原因也定位清楚了，知道问题的根源，解决起来自然也就是水到渠成的事情啦。

不过在解决BUG的过程中，由于相关代码的编写是由另外一位同事完成的，代码风格与规范也是各有一番千秋，简单地说就是，你要先看懂他写的代码并明白他的代码逻辑设计意图，才敢放手去改。不然保不齐又会引入其它新的问题。心有猛虎，细嗅蔷薇，小心驶得万年船啊！

问题从产生到发现到解决，其实过程并不是很长，但直到我最后一次上完线，修改完这个bug之后，再仔细回过头想一想这个事情，发现还是有很多值得思考与反思的地方：
1. 插入式需求的完成度强依赖于对于业务的熟悉程度，贸然去改一个之前不是很熟悉的业务，其实是很危险的一件事
2. 代码的规范程度之于BUG的修改、他人的阅读、扩展性编写的重要性，殊不知风格迥异的代码简直是对后续维护者的折磨
3. 产品需求的细节沟通程度决定了这个需求的完善程度，有的时候沟通的缺乏轻则导致需求设计的完善度，重则产生致命的漏洞bug。

## 写在最后

问题其实不算什么，但是透过这次问题确实让我感受比较五味杂陈，你说不出是哪里不爽，但觉得这个世界其实没有那么善良。总而言之，当好一名代码工程师确实不是一件容易的事情！
